{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# import packages #######################################################\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## read the hapmap file as a df ###############################################\n",
    "\n",
    "df = pd.read_csv('PATSA_w_IDs.hmp', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# list of plots to be removed from the df based on QC ##################################\n",
    "\n",
    "\n",
    "####### summer plots #######\n",
    "\n",
    "fil_col_summ= [\"BIMHA_Summer_CBI-Site-1_1_001\", \"LUKANGA_Summer_CBI-Site-5_1_111\", \"LUKANGA_Summer_SEEDCO-Site-6_2_322\", \n",
    "          \"MAKWACHA_Summer_SEEDCO-Site-5_2_289\", \"MHEMBWE_Summer_SEEDCO-Site-6_1_325\", \"MHEMBWE_Summer_SEEDCO-Site-6_2_326\",\n",
    "          \"MHOFU_Summer_CBI-Site-2_2_047\", \"SC_EXPT1-5-25_Summer_SEEDCO-Site-1_1_192\", \"SC_EXPT1-5-25_Summer_SEEDCO-Site-2_2_227\",\n",
    "          \"SC_EXPT1-5-25_Summer_SEEDCO-Site-3_1_262\", \"SC_EXPT3-5-54_Summer_CBI-Site-2_2_056\", \"SC_EXPT3-5-54_Summer_SEEDCO-Site-5_2_301\",\n",
    "          \"SC_SAGA_Summer_CBI-Site-5_1_127\", \"SC_SIGNAL_Summer_SEEDCO-Site-5_2_305\", \"SC_SPIKE_Summer_CBI-Site-2_2_062\",\n",
    "          \"SC_SPIKE_Summer_CBI-Site-5_1_131\", \"SC_SPIKE_Summer_SEEDCO-Site-5_2_307\",\"TGX_1987-62F_Summer_CBI-Site-2_1_063\",\n",
    "               \"TGX_1987-62F_Summer_CBI-Site-4_1_098\",\"TGX_1987-62F_Summer_CBI-Site-5_2_134\",\n",
    "          \"TGX_1987-62F_Summer_SEEDCO-Site-3_1_274\", \"TGX_1987-62F_Summer_SEEDCO-Site-6_1_343\", \"TGX_1991-22F_Summer_CBI-Site-2_2_066\",\n",
    "          \"TGX_1991-22F_Summer_SEEDCO-Site-1_1_206\", \"TGX_1991-22F_Summer_SEEDCO-Site-1_2_207\", \"TGX_2014-16FM_Summer_SEEDCO-Site-3_1_278\",\n",
    "          \"TIKOLORE_Summer_CBI-Site-2_1_070\", \"TIKOLORE_Summer_CBI-Site-4_2_106\", \"TIKOLORE_Summer_CBI-Site-5_1_140\", \n",
    "          \"TIKOLORE_Summer_SEEDCO-Site-1_1_210\", \"TGX_2014-16FM_Summer_CBI-Site-1_1_032\",\"TGX_2014-16FM_Summer_CBI-Site-1_2_033\",\n",
    "          \"SC_SAGA_Summer_CBI-Site-4_1_092\", \"SC_SAGA_Summer_CBI-Site-4_2_093\", \"SC_EXPT2-5-22_Summer_SEEDCO-Site-3_1_264\",\n",
    "           \"SC_EXPT2-5-22_Summer_SEEDCO-Site-3_2_265\"]\n",
    "\n",
    "####### winter plots #######\n",
    "\n",
    "fil_col_wint = [\"BIMHA_Winter_Chisumbanje_1_142\",\"BIMHA_Winter_Chisumbanje_2_143\", \"KALEYA_Winter_Shamva_1_354\",\n",
    "               \"SC_EXPT1-5-25_Winter_Shamva_1_365\", \"SC_EXPT2-5-22_Winter_Chisumbanje_1_159\", \"SC_EXPT2-5-22_Winter_Shamva_1_367\",\n",
    "               \"SC_SIGNAL_Winter_Shamva_1_373\", \"SC_SPIKE_Winter_Chisumbanje_2_168\",\"TGX_1991-22F_Winter_Chisumbanje_2_172\",\n",
    "               \"KALEYA_Winter_Chisumbanje_1_144\",\"KALEYA_Winter_Chisumbanje_2_145\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## create one list of plots to be filtered #######################################\n",
    "\n",
    "fil_col = fil_col_summ+fil_col_wint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### drop the plots from the dataframe ##############################################\n",
    "\n",
    "df_fil = df.drop(fil_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## subset overall df to make indivdual df for each genotype ##########################\n",
    "\n",
    "text_list = ['BIMHA', 'KALEYA', 'LUKANGA', 'MAKWACHA','MHEMBWE', 'MHOFU', 'SC_EXPT1', 'SC_EXPT2', 'SC_EXPT3', 'SC_SAGA',\n",
    "             'SC_SIGNAL', 'SC_SPIKE','TGX_1987', 'TGX_1991', 'TGX_2014', 'TIKOLORE', \"NASOKO\", \"S1079\", \"TGX_2002\"]\n",
    "\n",
    "for text in text_list:\n",
    "    df_subset = df_fil.iloc[:, :11].join(df_fil.filter(regex=text))\n",
    "    globals()[f'df_{text}'] = df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# print the name of all the dfs to see if everything worked correctly #################\n",
    "\n",
    "for var_name in locals().keys():\n",
    "    if var_name.startswith('df_'):\n",
    "        print(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# create a list of tuples (individual dfs and their column starts) ######################\n",
    "\n",
    "# create a list of dataframes and their corresponding column starts\n",
    "dfs_cols = [\n",
    "    (df_BIMHA, 'BIMHA_'),\n",
    "    (df_KALEYA, 'KALEYA_'),\n",
    "    (df_MHEMBWE, 'MHEMBWE_'),\n",
    "    (df_LUKANGA, 'LUKANGA_'),\n",
    "    (df_MHOFU, 'MHOFU_'),\n",
    "    (df_NASOKO, 'NASOKO_'),\n",
    "    (df_S1079, 'S1079'),\n",
    "    (df_SC_EXPT1, 'SC_EXPT1'),\n",
    "    (df_SC_EXPT2, 'SC_EXPT2'),\n",
    "    (df_SC_EXPT3, 'SC_EXPT3'),\n",
    "    (df_SC_SAGA, 'SC_SAGA_'),\n",
    "    (df_SC_SIGNAL, 'SC_SIGNAL_'),\n",
    "    (df_SC_SPIKE, 'SC_SPIKE_'),\n",
    "    (df_TGX_1987, 'TGX_1987'),\n",
    "    (df_TGX_1991, 'TGX_1991'),\n",
    "    (df_TGX_2002, 'TGX_2002'),\n",
    "    (df_TGX_2014, 'TGX_2014'),\n",
    "    (df_TIKOLORE, 'TIKOLORE_'),\n",
    "    (df_MAKWACHA, 'MAKWACHA_')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### replace chracters for heterozygotes by H ################################\n",
    "\n",
    "n=0\n",
    "for i, (df, col_start) in enumerate(dfs_cols):\n",
    "    cols = [col for col in df.columns if col.startswith(col_start)]\n",
    "    for col in cols:\n",
    "        df.loc[:, col] = df[col].apply(lambda x: 'H' if x not in ['A', 'T', 'C', 'G', 'N'] else x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### remove snp positions with more 90% missing ################################        \n",
    "\n",
    "for df, prefix in dfs_cols:\n",
    "    columns_to_check = [col for col in df.columns if col.startswith(prefix)]\n",
    "    df.drop(df[df[columns_to_check].eq('N').sum(axis=1) / len(columns_to_check) > 0.9].index, inplace=True)\n",
    "    df[columns_to_check] = df[columns_to_check].replace('N', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### functions to create consensus genotypes ################################        \n",
    "\n",
    "\n",
    "\n",
    "def add_consensus (dataframe, df_start):\n",
    "    # create a list of standard nucleotides\n",
    "    nucleotides = ['A', 'T', 'C', 'G']\n",
    "\n",
    "    # loop over the rows and columns starting with \"df_start value\"\n",
    "    for idx, row in dataframe.loc[:, dataframe.columns.str.startswith(df_start)].iterrows():\n",
    "        \n",
    "        # get the row-wise value counts for each standard nucleotide and H in the current row\n",
    "        n_counts = row.str.extractall('([ATCGH])')[0].value_counts()\n",
    "        \n",
    "        if n_counts[n_counts.index.isin(nucleotides)].shape[0] == 2:\n",
    "            print(n_counts)\n",
    "            print(n_counts[n_counts.index.isin(nucleotides)].max() - n_counts[n_counts.index.isin(nucleotides)].min())\n",
    "            print(n_counts.sum() * 0.15)\n",
    "    \n",
    "        # check if H is more than 80% frequent, otherwise check if there are two standard nucleotides in the row\n",
    "        if 'H' in n_counts.index and n_counts['H'] / n_counts.sum() > 0.8:\n",
    "            consensus = 'H'\n",
    "        \n",
    "        elif n_counts[n_counts.index.isin(nucleotides)].shape[0] == 2:\n",
    "            freq_diff = n_counts[n_counts.index.isin(nucleotides)].max() - n_counts[n_counts.index.isin(nucleotides)].min()\n",
    "            \n",
    "            # if the frequency difference is equal to or more than 15%, assign the maximum nucleotide as consensus, otherwise assign H\n",
    "            if freq_diff > n_counts.sum() * 0.15:\n",
    "                consensus = n_counts[n_counts.index.isin(nucleotides)].idxmax()\n",
    "            else:\n",
    "                consensus = 'H'\n",
    "        \n",
    "        else:\n",
    "            consensus = n_counts[n_counts.index.isin(nucleotides)].idxmax()\n",
    "\n",
    "        # assign the consensus value to the new column \"\"df_start\"_consensus\" for the current row\n",
    "        dataframe.loc[idx, df_start+'consensus'] = consensus\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, prefix in dfs_cols:\n",
    "    df = add_consensus(df, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### create a list of dfs to join back #######################################\n",
    "\n",
    "dfs_to_join = [df_BIMHA, df_KALEYA, df_MHEMBWE, df_LUKANGA, df_MHOFU, df_NASOKO,\n",
    "                        df_S1079, df_SC_EXPT1, df_SC_EXPT2, df_SC_EXPT3, df_SC_SAGA, df_SC_SIGNAL,\n",
    "                        df_SC_SPIKE, df_TGX_1987, df_TGX_1991, df_TGX_2002, df_TGX_2014, df_TIKOLORE, df_MAKWACHA]\n",
    "\n",
    "\n",
    "################################################### columns to join dfs ###########################################\n",
    "join_cols = ['RS', 'REF', 'Chr', 'POSITION', 'STRAND', 'ASSEMBLY', 'CENTER', 'PROTLSID', 'ASSAYLSID', 'PANELLSID', 'QCCODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### join dfs by join_cols to create a merged_df #############################\n",
    "\n",
    "merged_df = dfs_to_join[0]\n",
    "\n",
    "for i in range(1, len(dfs_to_join)):\n",
    "    merged_df = merged_df.merge(dfs_to_join[i], on=join_cols, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## only retain the consensus genotypes in final df ######################################\n",
    "\n",
    "columns_to_keep = join_cols + [col for col in merged_df.columns if col.endswith('consensus')]\n",
    "\n",
    "final_df = merged_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### starting to conver the df to vcf format #############################################\n",
    "\n",
    "############ add format column and replace Nans with N #################\n",
    "\n",
    "final_df['FORMAT'] = 'GT'\n",
    "\n",
    "consensus_cols = final_df.columns[final_df.columns.str.endswith('consensus')]\n",
    "\n",
    "final_df[consensus_cols] = final_df[consensus_cols].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### read in the file with version conversion info #########################\n",
    "\n",
    "df_1 = pd.read_csv(\"6K_SNP_ID_v2.txt\" , sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ merge the final df with snp version df (df_1) ########################\n",
    "\n",
    "final_df = final_df.merge(df_1, on=\"RS\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### extract correct alternate for each snp based on updated version\n",
    "\n",
    "final_df['ALT']=final_df[\"SNP_ID_V2\"].str.extract(r'([A-Z]$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## drop the columns not needed for vcf #####################################\n",
    "\n",
    "final_df = final_df.drop(['SNP_ID_V2', 'SNP_ID_V1', 'STRAND', 'ASSEMBLY', 'CENTER', 'PROTLSID', 'ASSAYLSID',\n",
    "                          'PANELLSID', 'POSITIONS', 'Chr'], axis=1)\n",
    "\n",
    "final_df = final_df.drop(['QCCODE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## fill in columns with \".\", necesarry for vcf format #####################\n",
    "\n",
    "final_df['QUAL'] = '.'\n",
    "final_df['FILTER'] = '.'\n",
    "final_df['INFO'] = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## arrange the columns according to vcf format ############################\n",
    "\n",
    "new_column_order = ['Chromo', 'POSITION', 'RS','REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', \n",
    "                    'BIMHA_consensus', 'KALEYA_consensus',\n",
    "                    'MHEMBWE_consensus', 'LUKANGA_consensus', 'MHOFU_consensus',\n",
    "                    'NASOKO_consensus', 'S1079consensus', 'SC_EXPT1consensus',\n",
    "                    'SC_EXPT2consensus', 'SC_EXPT3consensus', 'SC_SAGA_consensus',\n",
    "                    'SC_SIGNAL_consensus', 'SC_SPIKE_consensus', 'TGX_1987consensus',\n",
    "                    'TGX_1991consensus', 'TGX_2002consensus', 'TGX_2014consensus',\n",
    "                    'TIKOLORE_consensus', 'MAKWACHA_consensus']\n",
    "\n",
    "\n",
    "final_df = final_df.loc[:, new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### assing column names needed for vcf format ############################\n",
    "\n",
    "final_df.rename(columns={'Chromo': \"#CHROM\"}, inplace=True)\n",
    "final_df.rename(columns={'POSITION': \"POS\"}, inplace=True)\n",
    "final_df.rename(columns={'RS': \"ID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## change snp values to correspond with vcf format ######################\n",
    "\n",
    "for col in final_df.columns:\n",
    "    if col.endswith(\"consensus\"):\n",
    "        for idx, row in final_df.iterrows():\n",
    "            ref = row[\"REF\"]\n",
    "            alt = row[\"ALT\"]\n",
    "            val = row[col]\n",
    "            if val == \"H\":\n",
    "                final_df.at[idx, col] = \"0/1\"\n",
    "            elif val == \"N\":\n",
    "                final_df.at[idx, col] = \"./.\"\n",
    "            elif val == ref:\n",
    "                final_df.at[idx, col] = \"0/0\"\n",
    "            elif val == alt:\n",
    "                final_df.at[idx, col] = \"1/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### replace the way chromosomes are named ####################################\n",
    "\n",
    "final_df[\"#CHROM\"] = final_df[\"#CHROM\"].str.replace('Gm','Chr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### remove consensus designations ############################################\n",
    "\n",
    "for col in final_df.columns:\n",
    "    if col.endswith(\"_consensus\"):\n",
    "        final_df.rename(columns={col: col.rstrip(\"_consensus\")}, inplace=True)\n",
    "    elif col.endswith(\"consensus\"):\n",
    "        final_df.rename(columns={col: col.rstrip(\"consensus\")}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"PATSA_Consensus.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['vcf_headers.csv', 'PATSA_Consensus.csv']\n",
    " \n",
    "# Open file3 in write mode\n",
    "with open('PATSA_Consensus.vcf', 'w') as outfile:\n",
    "  \n",
    "    # Iterate through list\n",
    "    for names in filenames:\n",
    "  \n",
    "        # Open each file in read mode\n",
    "        with open(names) as infile:\n",
    "  \n",
    "            # read the data from file1 and\n",
    "            # file2 and write it in file3\n",
    "            outfile.write(infile.read())\n",
    "  \n",
    "        # Add '\\n' to enter data of file2\n",
    "        # from next line\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
